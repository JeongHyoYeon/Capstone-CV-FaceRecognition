{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-onsaBfxFpiyMNf8lOd3RHG8Mxxc-zDM","timestamp":1679809275020},{"file_id":"https://gist.github.com/vinotharjun/37c06cfaa2772e26ad7c8ad84705efe6#file-face-grouping-ipynb","timestamp":1677471591903}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["images = [\n","        {\n","            \"id\": 19,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/95c7716f-2610-4ae5-a37f-812f694b96d8\"\n","        },\n","        {\n","            \"id\": 20,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/d525a2df-1e15-42c3-beef-1091dd80cb01\"\n","        },\n","        {\n","            \"id\": 21,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/cf486a5a-2654-400e-9eea-e292f3ae7148\"\n","        },\n","        {\n","            \"id\": 22,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/5d90e8ad-550c-4bcf-b42f-807449219f11\"\n","        },\n","        {\n","            \"id\": 23,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/e28953c8-0bd0-44b8-9514-253bf87a02bc\"\n","        },\n","        {\n","            \"id\": 24,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/fbf99922-1aa7-4e3f-b5ec-276a8c61da79\"\n","        },\n","        {\n","            \"id\": 25,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/9c4d72bb-fb5e-4414-b983-f2f30b55725a\"\n","        },\n","        {\n","            \"id\": 26,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/d15b367c-501b-4282-b5fe-5953f1cb5b99\"\n","        },\n","        {\n","            \"id\": 27,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/969f761c-1431-4fd3-ba58-d452bd1184eb\"\n","        },\n","        {\n","            \"id\": 28,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/90e01d7c-81bc-45dd-9736-eb0c0d626639\"\n","        },\n","        {\n","            \"id\": 29,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/67607945-90d2-48ce-8b88-a6e40e34aac4\"\n","        },\n","        {\n","            \"id\": 30,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/3b70edc0-9c1c-4acf-800d-4f8eb9a520ca\"\n","        },\n","        {\n","            \"id\": 31,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/12864258-2f9c-45c6-8708-7a2748db42c3\"\n","        },\n","        {\n","            \"id\": 32,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/043119aa-f025-4ee1-b28f-8b219cc409d9\"\n","        },\n","        {\n","            \"id\": 33,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/dc792047-94b6-4910-bff1-5a860209cd07\"\n","        },\n","        {\n","            \"id\": 34,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/21f1f4b2-dbbe-4041-a766-2bd03e468074\"\n","        },\n","        {\n","            \"id\": 35,\n","            \"url\": \"https://capstone-aftertrip-test.s3.ap-northeast-2.amazonaws.com/40f8098b-0b21-4fe6-afc0-156ebfb7c2cf\"\n","        }\n","]"],"metadata":{"id":"O9dFFVMHBd0h","executionInfo":{"status":"ok","timestamp":1681405348312,"user_tz":-540,"elapsed":390,"user":{"displayName":"임연우","userId":"08836425153877533305"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#----------------------------------------------------------\n","#                          drive mount \n","#----------------------------------------------------------\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","#%cd /content/drive/MyDrive/\n","#!git clone https://github.com/vinotharjun/FaceGrouping.git\n","\n","\n","#----------------------------------------------------------\n","#                          install \n","#----------------------------------------------------------\n","base_folder = '/content/drive/MyDrive/Capstone_FaceRecognition'\n","%cd {base_folder}\n","\n","!pip install mtcnn\n","!pip install -r requirements.txt #embedding & tsne 위한 설치"],"metadata":{"id":"tFH_Ugl67NhG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681404363612,"user_tz":-540,"elapsed":72945,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"abafc96e-b56e-420b-e16c-35938081504e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Capstone_FaceRecognition\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mtcnn\n","  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from mtcnn) (2.12.0)\n","Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from mtcnn) (4.7.0.72)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.22.4)\n","Installing collected packages: mtcnn\n","Successfully installed mtcnn-0.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting matplotlib==3.3.0\n","  Downloading matplotlib-3.3.0.tar.gz (38.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy==1.19.0\n","  Downloading numpy-1.19.0.zip (7.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.3.0.36 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.3.0.36\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#----------------------------------------------------------\n","#                          import \n","#----------------------------------------------------------\n","import os\n","import time\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","from urllib import request\n","\n","from io import BytesIO\n","from google.colab.patches import cv2_imshow\n","import urllib.request\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.utils.data as data\n","import torchvision.datasets as datasets\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","\n","from backbone import Backbone\n","from mtcnn.mtcnn import MTCNN\n","from sklearn.cluster import DBSCAN\n","from sklearn.preprocessing import scale\n","\n","\n","#----------------------------------------------------------\n","#                      global variable\n","#----------------------------------------------------------f\n","#group_name = 'group0' #여기서 group명\n","#images_folder = base_folder + '/images'\n","#group_folder = images_folder + '/' + group_name\n","#aligned_group_folder = group_folder + '_crop'\n","#path_dict = {}\n","url_dict = {}\n","faces = []\n","groups = []\n","\n","input_size = 112\n","group_name = 'group0'\n","crop_file_num = 0\n","crop_base_folder = \"/content/drive/MyDrive/Capstone_FaceRecognition/crop\"\n","crop_folder = f\"/content/drive/MyDrive/Capstone_FaceRecognition/crop/{group_name}\"\n","cos_distance_threshold = 0.45 \n","is_already_in_group = False\n","\n","\n","#----------------------------------------------------------\n","#                        functions \n","#----------------------------------------------------------\n","def face_crop(img_idx, pixels, required_size=(112, 112)): \n","    \"\"\"\n","    MTCNN 모델을 이용해 Face Detection 하는 function\n","\n","    Args:\n","      img_idx (int):\n","      pixels (list (몇바이몇이지?)):\n","      required_size (tuple) :\n","\n","    Returns:\n","      None\n","\n","    Details:\n","      input: 이미지 1개의 pixel\n","      action: MTCNN 적용해서 face detection (alignment는 안된다)\n","      output: _crop 폴더에 detect한 이미지 crop해서 저장한다\n","    \"\"\"\n","\n","    # FaceDetection, 얼굴탐지\n","    # MTCCN 모델 이용\n","    detector = MTCNN()\n","    results = detector.detect_faces(pixels)\n","\n","    # 얼굴이 없는 이미지\n","    if len(results)==0:\n","      images[img_idx][\"face_idx\"] = [0]\n","\n","    # 얼굴이 1개이상인 이미지 \n","    crop_file_num = 1\n","    for i in range(len(results)):\n","        x1, y1, width, height = results[i]['box']\n","        x1, y1 = abs(x1), abs(y1)\n","        x2, y2 = x1 + width, y1 + height\n","        face = pixels[y1:y2, x1:x2]\n","        image = Image.fromarray(face)\n","        image = image.resize(required_size)\n","\n","        #  crop 폴더에 이미지 저장\n","        if not os.path.isdir(crop_base_folder):\n","          os.mkdir(crop_base_folder)\n","\n","        if not os.path.isdir(crop_folder):\n","          os.mkdir(crop_folder)\n","        \n","        crop_path = crop_folder + '/' + str(img_idx) + \"_\" + str(crop_file_num) + \".jpg\" \n","        crop_file_num = crop_file_num + 1\n","        print(\"crop_path = \", crop_path)\n","        image.save(crop_path)\n","\n","        #  dictionary를 이용해 crop_path와 original_image_idx mapping \n","        url_dict[crop_path] = img_idx\n","\n","\n","def alignment(): \n","    \"\"\"\n","    얼굴 각도 조절하는 함수 (미완성)\n","\n","    Args:\n","      \n","\n","    Returns:\n","      \n","\n","    Details:\n","  \n","    \"\"\"\n","\n","\n","def get_embeddings(data_root, model_root, input_size=[112, 112], embedding_size=512):\n","    \"\"\"\n","    data root에 들은 모든 이미지들을 Arcface 모델을 이용해 embedding하는 함수 \n","\n","    Args:\n","      data_root (str) : crop된 이미지들이 들어있는 폴더의 path (이 폴더 안에 하위폴더가 한개이상 존재해야함 ex)data_root/cat, data_root/dog)\n","      model_root (str) : arcface 모델의 .pth 파일의 path\n","\n","    Returns:\n","      faces (list): [{\n","                      \"crop_path\" : crop_path\n","                      \"face_idx\" : 전체 얼굴 중에서 몇번째 얼굴인지 (이게 all_faces의 idx랑 동일함)\n","                      \"face\" : [얼굴 pixel RGB] \n","                    }]\n","\n","      embeddings (list):[[embedding1], [embedding2] ... [embeddingN]]\n","    \"\"\"\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # data & model 경로\n","    assert os.path.exists(data_root)\n","    assert os.path.exists(model_root)\n","    print(f\"Data root: {data_root}\")\n","\n","    # define image preprocessing\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize(\n","                [int(128 * input_size[0] / 112), int(128 * input_size[0] / 112)],\n","            ),  # smaller side resized\n","            transforms.CenterCrop([input_size[0], input_size[1]]),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","        ],\n","    )\n","\n","    # define data loader\n","    dataset = datasets.ImageFolder(data_root, transform)\n","    loader = data.DataLoader(\n","        dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=0,\n","    )\n","    #print(f\"Number of classes: {len(loader.dataset.classes)}\")\n","\n","    # load backbone weigths from a checkpoint\n","    backbone = Backbone(input_size)\n","    backbone.load_state_dict(torch.load(model_root, map_location=torch.device(\"cpu\")))\n","    backbone.to(device)\n","    backbone.eval()\n","\n","    # get embedding for each face\n","    embeddings = np.zeros([len(loader.dataset), embedding_size])\n","    with torch.no_grad():\n","        for idx, (image, _) in enumerate(\n","            tqdm(loader, desc=\"Create embeddings matrix\", total=len(loader)),\n","        ):\n","            embeddings[idx, :] = F.normalize(backbone(image.to(device))).cpu()\n","\n","    # faces 에 crop_path, face_idx, face 저장\n","    i = 0\n","    for crop_path, _ in dataset.samples:\n","      face = cv2.imread(crop_path)\n","      faces.append(\n","          {\n","            \"crop_path\":crop_path, \n","            \"face_idx\" : i,\n","            \"face\":face\n","          }\n","      )\n","      i = i + 1\n","\n","    return faces, embeddings"],"metadata":{"id":"FXVA_Qx-FSR4","executionInfo":{"status":"ok","timestamp":1681404384760,"user_tz":-540,"elapsed":18926,"user":{"displayName":"임연우","userId":"08836425153877533305"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#----------------------------------------------------------\n","#                       face detect\n","#----------------------------------------------------------\n","for idx,image in enumerate(images):\n","  #  s3에서 생성된 url을 request&response로 받아서 img로 넘기기\n","  res = request.urlopen(image[\"url\"]).read()\n","  img = Image.open(BytesIO(res))\n","  img = img.convert('RGB')\n","  pixels = np.asarray(img)  \n","\n","  #  face detect 얼굴 탐지\n","  face_crop(idx, pixels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cATFHsnwRMFY","executionInfo":{"status":"ok","timestamp":1681404454833,"user_tz":-540,"elapsed":64555,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"4671838f-f553-476d-d0b0-d1eaed978153"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 523ms/step\n","1/1 [==============================] - 0s 178ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 30ms/step\n","6/6 [==============================] - 0s 9ms/step\n","2/2 [==============================] - 0s 24ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/0_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/0_2.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/0_3.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/0_4.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/0_5.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/0_6.jpg\n","1/1 [==============================] - 0s 195ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ee6f16040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 156ms/step\n","1/1 [==============================] - 0s 45ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","11/11 [==============================] - 0s 11ms/step\n","2/2 [==============================] - 0s 11ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/1_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/1_2.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/1_3.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/1_4.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/1_5.jpg\n","1/1 [==============================] - 0s 171ms/step\n","1/1 [==============================] - 0s 175ms/step\n","1/1 [==============================] - 0s 73ms/step\n","1/1 [==============================] - 0s 49ms/step\n","1/1 [==============================] - 0s 43ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 56ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 37ms/step\n","7/7 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 280ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/2_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/2_2.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/2_3.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/2_4.jpg\n","1/1 [==============================] - 0s 179ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ee5e190d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 150ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","12/12 [==============================] - 0s 9ms/step\n","2/2 [==============================] - 0s 16ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/3_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/3_2.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/3_3.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/3_4.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/3_5.jpg\n","1/1 [==============================] - 0s 132ms/step\n","1/1 [==============================] - 0s 113ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","2/2 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 158ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/4_1.jpg\n","1/1 [==============================] - 0s 124ms/step\n","1/1 [==============================] - 0s 129ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","3/3 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 179ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/5_1.jpg\n","1/1 [==============================] - 0s 171ms/step\n","1/1 [==============================] - 0s 169ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 35ms/step\n","2/2 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 258ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/6_1.jpg\n","1/1 [==============================] - 0s 117ms/step\n","1/1 [==============================] - 0s 109ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","2/2 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 164ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/7_1.jpg\n","1/1 [==============================] - 0s 127ms/step\n","1/1 [==============================] - 0s 111ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","4/4 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 175ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/8_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/8_2.jpg\n","1/1 [==============================] - 0s 120ms/step\n","1/1 [==============================] - 0s 108ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 32ms/step\n","2/2 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 169ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/9_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/9_2.jpg\n","1/1 [==============================] - 0s 147ms/step\n","1/1 [==============================] - 0s 188ms/step\n","1/1 [==============================] - 0s 66ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 35ms/step\n","6/6 [==============================] - 0s 15ms/step\n","2/2 [==============================] - 0s 15ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/10_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/10_2.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/10_3.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/10_4.jpg\n","1/1 [==============================] - 0s 150ms/step\n","1/1 [==============================] - 0s 125ms/step\n","1/1 [==============================] - 0s 44ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","7/7 [==============================] - 0s 10ms/step\n","1/1 [==============================] - 0s 168ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/11_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/11_2.jpg\n","1/1 [==============================] - 0s 153ms/step\n","1/1 [==============================] - 0s 123ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 132ms/step\n","1/1 [==============================] - 0s 160ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/12_1.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/12_2.jpg\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/12_3.jpg\n","1/1 [==============================] - 0s 158ms/step\n","1/1 [==============================] - 0s 139ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","3/3 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 156ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/13_1.jpg\n","1/1 [==============================] - 0s 280ms/step\n","1/1 [==============================] - 0s 238ms/step\n","1/1 [==============================] - 0s 79ms/step\n","1/1 [==============================] - 0s 55ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 37ms/step\n","3/3 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 241ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/14_1.jpg\n","1/1 [==============================] - 0s 149ms/step\n","1/1 [==============================] - 0s 132ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","2/2 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 163ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/15_1.jpg\n","1/1 [==============================] - 0s 144ms/step\n","1/1 [==============================] - 0s 132ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 32ms/step\n","3/3 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 165ms/step\n","crop_path =  /content/drive/MyDrive/Capstone_FaceRecognition/crop/group0/16_1.jpg\n"]}]},{"cell_type":"code","source":["#----------------------------------------------------------\n","#                       embedding\n","#----------------------------------------------------------\n","#  get embeddings\n","faces, embeddings = get_embeddings(\n","        data_root = crop_base_folder ,\n","        model_root = \"checkpoint/backbone_ir50_ms1m_epoch120.pth\",\n","        input_size = [input_size, input_size],\n","      )\n","\n","print(\"faces 길이 = \", len(faces))\n","print(\"embeddings 길이 = \", len(embeddings)) \n","\n","#  faces에 embeddings 넣기\n","for i, embedding in enumerate(embeddings):\n","  faces[i][\"embedding\"] = embedding\n","\n","\n","#----------------------------------------------------------\n","#                      cos_similarity 계산\n","#----------------------------------------------------------\n","#  cosine_similarity[i][j]로 접근 가능\n","cosine_similaritys = np.dot(embeddings, embeddings.T)\n","cosine_similaritys = cosine_similaritys.clip(min=0, max=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgfZ0quz_AAE","executionInfo":{"status":"ok","timestamp":1681404479235,"user_tz":-540,"elapsed":17174,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"20dbe92c-5159-4ec2-d614-abed332676d3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Data root: /content/drive/MyDrive/Capstone_FaceRecognition/crop\n"]},{"output_type":"stream","name":"stderr","text":["Create embeddings matrix: 100%|██████████| 41/41 [00:13<00:00,  2.96it/s]"]},{"output_type":"stream","name":"stdout","text":["faces 길이 =  41\n","embeddings 길이 =  41\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#----------------------------------------------------------\n","#                       grouping\n","#----------------------------------------------------------\n","# face1 : 비교기준이 되는 이미지\n","# face2 : 지금 새로 그룹을 부여할 이미지\n","for face2_idx, face2 in enumerate(faces) : # 해당 얼굴에 대해\n","    is_already_in_group = False\n","\n","    # Case1 : 기존 그룹 탐색\n","    for group_idx, group in enumerate(groups): # 각 그룹마다\n","      for i, face1_idx in enumerate(group[\"face2_idx_list\"]) : #그 그룹에 들어있는 얼굴마다\n","        cosine_similarity = cosine_similaritys[face1_idx][face2_idx]\n","\n","        if( not is_already_in_group and cosine_similarity > cos_distance_threshold ) :\n","          crop_path = face2[\"crop_path\"]\n","          original_images_idx = url_dict[crop_path]\n","\n","          # images에 group idx 넣어주기\n","          if \"group_idx\" in images[original_images_idx] :\n","            images[original_images_idx][\"group_idx\"].append(group_idx)\n","          else :\n","            images[original_images_idx][\"group_idx\"] = [group_idx]\n","\n","          # groups 갱신\n","          group[\"original_images_idx_list\"].append(original_images_idx) #dictionary에서 원래 url idx 찾아넣기\n","          group[\"crop_path_list\"].append(crop_path)\n","          group[\"face_list\"].append(face2[\"face\"])\n","          group[\"face2_idx_list\"].append(face2_idx)\n","          group[\"face1_idx_list\"].append(face1_idx)\n","          group[\"cosine_similarity_list\"].append(cosine_similarity)\n","\n","          is_already_in_group = True\n","          \n","\n","    # Case2 : 못 넣었으면 새로운 그룹에 추가\n","    if not is_already_in_group :\n","      crop_path = face2[\"crop_path\"]\n","      original_images_idx = url_dict[crop_path]\n","\n","      # images에 group idx 넣어주기\n","      if \"group_idx\" in images[original_images_idx] :\n","        images[original_images_idx][\"group_idx\"].append(len(groups))\n","      else :\n","        images[original_images_idx][\"group_idx\"] = [len(groups)]\n","\n","      # groups 갱신\n","      groups.append ({\n","          #\"original_path_list\" : [path_dict[crop_path]], #dictionary에서 원래 path 찾아넣기\n","          \"original_images_idx_list\" :[original_images_idx],\n","          \"crop_path_list\" : [crop_path],\n","          \"face_list\" : [face2[\"face\"]],\n","          \"face2_idx_list\" : [face2_idx], \n","          \"face1_idx_list\" : [[]],\n","          \"cosine_similarity_list\" : [-1]\n","      })\n","\n","print(len(groups))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bca9RKoplHq6","executionInfo":{"status":"ok","timestamp":1681405364459,"user_tz":-540,"elapsed":4,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"f53fa2db-7226-4847-ea74-be75115c47d6"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["28\n"]}]},{"cell_type":"code","source":["#grouping 잘됐나 테스트\n","from google.colab.patches import cv2_imshow\n","\n","for i in range(len(groups)):\n","  print(\"======================================================\")\n","  print(\"======================group\", i, \"======================\")\n","  for j in range(len(groups[i][\"face_list\"])):\n","    print(\"face idx = \", groups[i][\"face2_idx_list\"][j]) #face idx\n","    print(\"cosine = \", groups[i][\"cosine_similarity_list\"][j]) #cosine distance\n","    #cv2_imshow(groups[i][\"face_list\"][j]) #face\n","\n","    crop_path = groups[i][\"crop_path_list\"][j]\n","    original_images_idx = groups[i][\"original_images_idx_list\"][j]\n","    \n","    print(\"crop_path = \", crop_path)\n","    cv2_imshow(cv2.imread(crop_path)) \n","  \n","    print(\"original_images_idx_list = \", original_images_idx)\n","    url = images[original_images_idx][\"url\"]\n","    resp = urllib.request.urlopen(url)\n","    image = np.asarray(bytearray(resp.read()), dtype='uint8')\n","    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n","\n","    cv2_imshow(image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dJELFXjQsSnZs9mDryeWn53EMWMcwAn8"},"id":"YHiUvaGa2FYn","executionInfo":{"status":"ok","timestamp":1681404540050,"user_tz":-540,"elapsed":49494,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"bbbce1bb-598f-43d2-8a85-e87e4177c3a0"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# group len이 1인건 group index 0 으로 변경\n","for group_idx, group in enumerate(groups):\n","  if len(group[\"original_images_idx_list\"]) == 1 :\n","    print(\"group idx = \", group_idx)\n","    images_idx = group[\"original_images_idx_list\"][0]\n","    images[images_idx][\"group_idx\"].remove(group_idx)\n","    if -1 not in images[images_idx][\"group_idx\"] :\n","      images[images_idx][\"group_idx\"].append(-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0Alup3vt6Qq","executionInfo":{"status":"ok","timestamp":1681405373081,"user_tz":-540,"elapsed":433,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"7b1b856b-9579-4b2d-cffd-a077cb1e987b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["group idx =  0\n","group idx =  1\n","group idx =  2\n","group idx =  3\n","group idx =  6\n","group idx =  7\n","group idx =  9\n","group idx =  11\n","group idx =  12\n","group idx =  14\n","group idx =  15\n","group idx =  16\n","group idx =  17\n","group idx =  18\n","group idx =  19\n","group idx =  20\n","group idx =  21\n","group idx =  22\n","group idx =  23\n","group idx =  24\n","group idx =  25\n","group idx =  26\n","group idx =  27\n"]}]},{"cell_type":"code","source":["#  최종값 잘 들어갔나 확인\n","for _, image in enumerate(images):\n","  print(\"image idx = \", image[\"id\"])\n","  for _, group_idx in enumerate(image[\"group_idx\"]):\n","    print(group_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBSBTp4hO-Ds","executionInfo":{"status":"ok","timestamp":1681405377159,"user_tz":-540,"elapsed":404,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"159ed6c4-c1f7-462e-ec89-67ca07c7637b"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["image idx =  19\n","4\n","5\n","-1\n","image idx =  20\n","4\n","8\n","-1\n","image idx =  21\n","13\n","10\n","-1\n","image idx =  22\n","10\n","8\n","4\n","-1\n","image idx =  23\n","4\n","image idx =  24\n","4\n","image idx =  25\n","5\n","image idx =  26\n","-1\n","image idx =  27\n","-1\n","image idx =  28\n","-1\n","image idx =  29\n","8\n","4\n","-1\n","image idx =  30\n","8\n","-1\n","image idx =  31\n","10\n","-1\n","image idx =  32\n","13\n","image idx =  33\n","5\n","image idx =  34\n","-1\n","image idx =  35\n","-1\n"]}]}]}